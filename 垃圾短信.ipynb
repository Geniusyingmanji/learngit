{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b3e359c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>标签</th>\n",
       "      <th>短信内容</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>商业秘密的秘密性那是维系其商业价值和垄断地位的前提条件之一</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>带给我们大常州一场壮观的视觉盛宴</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>有原因不明的泌尿系统结石等</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>23年从盐城拉回来的麻麻的嫁妆</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>感到自减肥、跳减肥健美操、</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   标签                           短信内容\n",
       "0   0  商业秘密的秘密性那是维系其商业价值和垄断地位的前提条件之一\n",
       "1   0               带给我们大常州一场壮观的视觉盛宴\n",
       "2   0                  有原因不明的泌尿系统结石等\n",
       "3   0                23年从盐城拉回来的麻麻的嫁妆\n",
       "4   0                  感到自减肥、跳减肥健美操、"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_table(\"./data/noteData.txt\",sep='\\t',header=None,nrows=10000,names=[\"标签\",\"短信内容\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc971d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86308473",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>标签</th>\n",
       "      <th>短信内容</th>\n",
       "      <th>分词后数据</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>商业秘密的秘密性那是维系其商业价值和垄断地位的前提条件之一</td>\n",
       "      <td>商业秘密 的 秘密性 那 是 维系 其 商业价值 和 垄断 地位 的 前提条件 之一</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>带给我们大常州一场壮观的视觉盛宴</td>\n",
       "      <td>带给 我们 大 常州 一场 壮观 的 视觉 盛宴</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>有原因不明的泌尿系统结石等</td>\n",
       "      <td>有 原因 不明 的 泌尿系统 结石 等</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>23年从盐城拉回来的麻麻的嫁妆</td>\n",
       "      <td>23 年 从 盐城 拉回来 的 麻麻 的 嫁妆</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>感到自减肥、跳减肥健美操、</td>\n",
       "      <td>感到 自 减肥 、 跳 减肥 健美操 、</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   标签                           短信内容  \\\n",
       "0   0  商业秘密的秘密性那是维系其商业价值和垄断地位的前提条件之一   \n",
       "1   0               带给我们大常州一场壮观的视觉盛宴   \n",
       "2   0                  有原因不明的泌尿系统结石等   \n",
       "3   0                23年从盐城拉回来的麻麻的嫁妆   \n",
       "4   0                  感到自减肥、跳减肥健美操、   \n",
       "\n",
       "                                        分词后数据  \n",
       "0  商业秘密 的 秘密性 那 是 维系 其 商业价值 和 垄断 地位 的 前提条件 之一  \n",
       "1                    带给 我们 大 常州 一场 壮观 的 视觉 盛宴  \n",
       "2                         有 原因 不明 的 泌尿系统 结石 等  \n",
       "3                     23 年 从 盐城 拉回来 的 麻麻 的 嫁妆  \n",
       "4                        感到 自 减肥 、 跳 减肥 健美操 、  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "data['分词后数据']=data['短信内容'].apply(lambda x:' '.join(jieba.cut(x)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fb0f110",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('./data/my_stop_words.txt','r',errors='ignore')\n",
    "my_stop_words_data=f.readlines()\n",
    "f.close()\n",
    "my_stop_words_list=[]\n",
    "for each in my_stop_words_data:\n",
    "    my_stop_words_list.append(each.strip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acfe553f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0              商业秘密 的 秘密性 那 是 维系 其 商业价值 和 垄断 地位 的 前提条件 之一\n",
      "1                                带给 我们 大 常州 一场 壮观 的 视觉 盛宴\n",
      "2                                     有 原因 不明 的 泌尿系统 结石 等\n",
      "3                                 23 年 从 盐城 拉回来 的 麻麻 的 嫁妆\n",
      "4                                    感到 自 减肥 、 跳 减肥 健美操 、\n",
      "                              ...                        \n",
      "9995                                 明天 微软 Windows10 发布 啦\n",
      "9996                        我 分享 了 百度 云里 的 文件 ： ? 听力 xxxx\n",
      "9997    痛是 真的 痛 就是 觉得 彻底 彻底 的 痛 没 了解 生活 的 真相 而痛 失去 了 重...\n",
      "9998                                  微软 为了 重新 争夺 移动 市场份额\n",
      "9999                        第一 时间 拿 起 手机 点开 微信 领取 红包 一气呵成\n",
      "Name: 分词后数据, Length: 10000, dtype: object\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "9995    0\n",
      "9996    0\n",
      "9997    0\n",
      "9998    0\n",
      "9999    0\n",
      "Name: 标签, Length: 10000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X=data['分词后数据']\n",
    "y=data['标签']\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b358f16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yingmanji/opt/miniconda3/envs/d2l/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ôщ', 'īȼ', 'ĳщ', 'ǡǡ', 'ϊʲô', 'ҫô', 'ҫȼ', 'һщ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yingmanji/opt/miniconda3/envs/d2l/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ôщ', 'īȼ', 'ĳщ', 'ǡǡ', 'ϊʲô', 'ҫô', 'ҫȼ', 'һщ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yingmanji/opt/miniconda3/envs/d2l/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ôщ', 'īȼ', 'ĳщ', 'ǡǡ', 'ϊʲô', 'ҫô', 'ҫȼ', 'һщ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yingmanji/opt/miniconda3/envs/d2l/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ôщ', 'īȼ', 'ĳщ', 'ǡǡ', 'ϊʲô', 'ҫô', 'ҫȼ', 'һщ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yingmanji/opt/miniconda3/envs/d2l/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ôщ', 'īȼ', 'ĳщ', 'ǡǡ', 'ϊʲô', 'ҫô', 'ҫȼ', 'һщ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yingmanji/opt/miniconda3/envs/d2l/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ôщ', 'īȼ', 'ĳщ', 'ǡǡ', 'ϊʲô', 'ҫô', 'ҫȼ', 'һщ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yingmanji/opt/miniconda3/envs/d2l/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ôщ', 'īȼ', 'ĳщ', 'ǡǡ', 'ϊʲô', 'ҫô', 'ҫȼ', 'һщ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yingmanji/opt/miniconda3/envs/d2l/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ôщ', 'īȼ', 'ĳщ', 'ǡǡ', 'ϊʲô', 'ҫô', 'ҫȼ', 'һщ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yingmanji/opt/miniconda3/envs/d2l/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ôщ', 'īȼ', 'ĳщ', 'ǡǡ', 'ϊʲô', 'ҫô', 'ҫȼ', 'һщ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yingmanji/opt/miniconda3/envs/d2l/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ôщ', 'īȼ', 'ĳщ', 'ǡǡ', 'ϊʲô', 'ҫô', 'ҫȼ', 'һщ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.903\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "skf=StratifiedKFold(n_splits=10,random_state=1,shuffle=True)\n",
    "for train_index,test_index in skf.split(X,y):\n",
    "    X_train,X_test=X[train_index],X[test_index]\n",
    "    y_train,y_test=y[train_index],y[test_index]\n",
    "    \n",
    "    pipeline=Pipeline([\n",
    "        ('vect',TfidfVectorizer(stop_words=my_stop_words_list)),\n",
    "        ('clf',MultinomialNB(alpha=1.0))\n",
    "    ])\n",
    "    pipeline.fit(X_train,Y_train)\n",
    "    predict=pipeline.predict(X_test)\n",
    "    score=pipeline.score(X_test,y_test)\n",
    "    print(score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9417106e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>标签</th>\n",
       "      <th>短信内容</th>\n",
       "      <th>分词后数据</th>\n",
       "      <th>数据类型</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>商业秘密的秘密性那是维系其商业价值和垄断地位的前提条件之一</td>\n",
       "      <td>商业秘密 的 秘密性 那 是 维系 其 商业价值 和 垄断 地位 的 前提条件 之一</td>\n",
       "      <td>正常短信</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>带给我们大常州一场壮观的视觉盛宴</td>\n",
       "      <td>带给 我们 大 常州 一场 壮观 的 视觉 盛宴</td>\n",
       "      <td>正常短信</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>有原因不明的泌尿系统结石等</td>\n",
       "      <td>有 原因 不明 的 泌尿系统 结石 等</td>\n",
       "      <td>正常短信</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>23年从盐城拉回来的麻麻的嫁妆</td>\n",
       "      <td>23 年 从 盐城 拉回来 的 麻麻 的 嫁妆</td>\n",
       "      <td>正常短信</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>感到自减肥、跳减肥健美操、</td>\n",
       "      <td>感到 自 减肥 、 跳 减肥 健美操 、</td>\n",
       "      <td>正常短信</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   标签                           短信内容  \\\n",
       "0   0  商业秘密的秘密性那是维系其商业价值和垄断地位的前提条件之一   \n",
       "1   0               带给我们大常州一场壮观的视觉盛宴   \n",
       "2   0                  有原因不明的泌尿系统结石等   \n",
       "3   0                23年从盐城拉回来的麻麻的嫁妆   \n",
       "4   0                  感到自减肥、跳减肥健美操、   \n",
       "\n",
       "                                        分词后数据  数据类型  \n",
       "0  商业秘密 的 秘密性 那 是 维系 其 商业价值 和 垄断 地位 的 前提条件 之一  正常短信  \n",
       "1                    带给 我们 大 常州 一场 壮观 的 视觉 盛宴  正常短信  \n",
       "2                         有 原因 不明 的 泌尿系统 结石 等  正常短信  \n",
       "3                     23 年 从 盐城 拉回来 的 麻麻 的 嫁妆  正常短信  \n",
       "4                        感到 自 减肥 、 跳 减肥 健美操 、  正常短信  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"数据类型\"]=pipeline.predict(X)\n",
    "data[\"数据类型\"]=data[\"数据类型\"].apply(lambda x:\"垃圾短信\" if x==1 else \"正常短信\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d1e24c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
